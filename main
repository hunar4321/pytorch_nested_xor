import torch
from torch.autograd import Variable
import torch.nn as nn
#import torch.nn.functional as F
import torch.optim as optim
import random

def gen_xor(x1, x2):
  if(x1==0 and x2==1):
    return 1
  if(x1==1 and x2==0):
    return 1
  else:
    return 0
  
def nested_xor(xs):
  ys = []
  
  ys.append(gen_xor(xs[0], xs[1]))
  ys.append(gen_xor(xs[2], xs[3]))
  ys.append(gen_xor(xs[4], xs[5]))
  ys.append(gen_xor(xs[6], xs[7]))
  
  out1 = gen_xor(ys[0], ys[1])
  out2 = gen_xor(ys[2], ys[3])
  
  out = gen_xor(out1, out2)
  
  return out

EPOCHS_TO_TRAIN = 5000

one_hidden = False;

class Net(nn.Module):   

    def __init__(self):
        super(Net, self).__init__()
        
        _input = 2 
        _output = 1 
        
        if(one_hidden == True):
          
          hidden1 = 2

          self.fc1 = nn.Linear(_input, hidden1, True)
          self.fc2 = nn.Linear(hidden1, _output, True)          
          print("One hidden layer, total params:", _input*hidden1 + hidden1*_output)
          
        else:

          hidden1 = 2
          hidden2 = 3

          self.fc1 = nn.Linear(_input, hidden1, True)
          self.fc2 = nn.Linear(hidden1, hidden2, True)
          self.fc3 = nn.Linear(hidden2, _output, True)
          print("Two hidden layers, total_params:", _input*hidden1 + hidden1*hidden2 + hidden2* _output)
          

    def forward(self, x):
        x = torch.sigmoid(self.fc1(x))
        if(one_hidden == False):
          x = torch.sigmoid(self.fc2(x))
          x = self.fc3(x)
        else:
          x = self.fc2(x)
        return x

# Xdata = [
#     [0, 0],
#     [0, 1],
#     [1, 0],
#     [1, 1]
# ]

# Ydata = [
#     [0],
#     [1],
#     [1],
#     [0]
# ]

Xdata=[]
Ydata=[]
for i in range(1000):
  Xdata.append([])
  for j in range(8):
    Xdata[i].append(round(random.random())
  Ydata.append(nested_xor(Xdata[i]))
        
net = Net()
inputs = list(map(lambda s: Variable(torch.Tensor([s])), Xdata))
targets = list(map(lambda s: Variable(torch.Tensor([s])), Ydata))


criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.1)

print("Training loop:")
for idx in range(0, EPOCHS_TO_TRAIN):
    for input, target in zip(inputs, targets):
        optimizer.zero_grad()   # zero the gradient buffers
        output = net(input)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()    # Does the update
    if idx % 1000 == 0:
        print("Epoch {: >8} Loss: {}".format(idx, loss.data.numpy()))

print("")
print("Final results:")
for input, target in zip(inputs, targets):
    output = net(input)
    print("Input:[{},{}] Target:[{}] Predicted:[{}] Error:[{}]".format(
        int(input.data.numpy()[0][0]),
        int(input.data.numpy()[0][1]),
        int(target.data.numpy()[0]),
        round(float(output.data.numpy()[0]), 4),
        round(float(abs(target.data.numpy()[0] - output.data.numpy()[0])), 4)
))


